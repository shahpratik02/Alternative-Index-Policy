{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.activations import relu, linear\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import math\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from deadlineSchedulingEnv import deadlineSchedulingEnv\n",
    "import concurrent.futures\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.13.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "MEMORY_SIZE = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.9995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim=3\n",
    "action_dim=2\n",
    "intermediate_dim=16\n",
    "T=13\n",
    "B=10\n",
    "processing_costs=[0.1,0.3,0.6,0.8]\n",
    "num_states=T*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel:\n",
    "    def __init__(self, input_dim, output_dim, lr):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.Qpolicy = self.create()\n",
    "        self.Qtarget = self.create() \n",
    "        self.Qtarget.set_weights(self.Qpolicy.get_weights())\n",
    "        \n",
    "    def create(self):\n",
    "        model = Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(1,state_dim)))\n",
    "        model.add(Dense(512,activation = 'relu'))\n",
    "        model.add(Dense(256, activation = 'relu'))\n",
    "        model.add(Dense(128, activation = 'relu'))\n",
    "        model.add(Dense(self.output_dim, activation = 'linear'))\n",
    "        # model.compile(optimizer = RMSprop(learning_rate = self.lr, rho = 0.95, epsilon = 1e-7), loss = \"mse\", metrics = ['accuracy'])\n",
    "        model.compile(optimizer =tf.keras.optimizers.legacy.Adam(learning_rate=self.lr), loss = \"mse\", metrics = ['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "    def __init__(self, all_states,state_space,action_space, decay_coe = 0.99, \n",
    "                  memory_size = MEMORY_SIZE,EXPLORATION_DECAY=EXPLORATION_DECAY,LEARNING_RATE=LEARNING_RATE,EPSILON_MAX=EPSILON_MAX,EPSILON_MIN=EPSILON_MIN,BATCH_SIZE=BATCH_SIZE):\n",
    "    \n",
    "        self.all_states=all_states\n",
    "        self.states = state_space\n",
    "        self.n_actions = action_space\n",
    "        \n",
    "        self.actions = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.lr = LEARNING_RATE\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = EPSILON_MAX\n",
    "        self.decay_coe = decay_coe\n",
    "        self.min_eps = EPSILON_MIN\n",
    "        #self.episodes = episodes\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.memory = deque(maxlen = memory_size) # replay memory \n",
    "        \n",
    "        self.terminal_state = False # end of the episode\n",
    "        self.target_counter = 0 \n",
    "        self.exploration_decay=EXPLORATION_DECAY\n",
    "        # Plot data\n",
    "        #self.timestep = self.episodes / 10\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model = QModel((1,self.states), self.n_actions, self.lr)\n",
    "        # Smooth epsilon \n",
    "        # self.a = 0.35\n",
    "        # self.b = 0.1\n",
    "        # self.c = 0.01\n",
    "        \n",
    "    def state_shape(self,states):\n",
    "        states = np.array(states)\n",
    "        return states.reshape(-1,*states.shape)\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Updates the current target_q_net with the q_net which brings all the\n",
    "        training in the q_net to the target_q_net.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.model.Qtarget.set_weights(self.model.Qpolicy.get_weights())\n",
    "    def decrement_epsilon(self):\n",
    "        '''\n",
    "        if self.epsilon > self.min_eps:\n",
    "            self.epsilon *= self.decay_coe\n",
    "        else:\n",
    "            self.epsilon = self.min_eps\n",
    "        '''\n",
    "        # s_time = (time - self.a*self.episodes) / (self.b*self.episodes) \n",
    "        # cosh = np.cosh(math.exp(-s_time))\n",
    "        # self.epsilon = 1 - (1/cosh + (time*self.c/self.episodes))\n",
    "        # if self.epsilon>self.min_eps:\n",
    "        self.epsilon*=self.exploration_decay\n",
    "        # else:\n",
    "        #     self.epsilon=self.min_eps\n",
    "    def forget(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    def remember(self, s, a, r, s_, done,subsidy):\n",
    "        self.memory.append([self.state_shape(s), a, r, self.state_shape(s_), done,subsidy])\n",
    "        \n",
    "    def act(self, states):\n",
    "        if np.random.random() > (1 - self.epsilon):\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            states = self.state_shape(states)\n",
    "            states.reshape(1,1,self.states)\n",
    "#             states=[states]\n",
    "#             states=np.array(states)\n",
    "            #print(states.shape)\n",
    "            action = np.argmax(np.array(self.model.Qpolicy.predict_on_batch(states)))\n",
    "            \n",
    "        return action\n",
    "            \n",
    "    def minibatch(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "\n",
    "        \n",
    "    def train(self):\n",
    "        # X - states passed to the NN, y - target\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        if len(self.memory) >= self.batch_size: \n",
    "            SARS = self.minibatch()\n",
    "        \n",
    "            s = self.state_shape([row[0] for row in SARS])\n",
    "            s=s.reshape(BATCH_SIZE,1,state_dim)\n",
    "            #print(s.shape)\n",
    "            qvalue = np.array(self.model.Qpolicy.predict_on_batch(s))\n",
    "            #print(qvalue)\n",
    "\n",
    "            s_ = self.state_shape([row[3] for row in SARS])\n",
    "            s_=s_.reshape(BATCH_SIZE,1,state_dim)\n",
    "            future_qvalue = np.array(self.model.Qtarget.predict_on_batch(s_))\n",
    "            #print(future_qvalue)\n",
    "\n",
    "            for index, (state, action, reward, state_, done,subsidy) in enumerate(SARS):\n",
    "                if done == True:\n",
    "                    Qtarget = reward +(1-action)*(subsidy)\n",
    "                else:\n",
    "                    Qtarget = reward +(1-action)*(subsidy) + np.max(future_qvalue[index][0])-np.array(self.model.Qpolicy.predict_on_batch((self.state_shape(self.all_states)).reshape(len(self.all_states),1,state_dim))).sum()/(len(self.all_states)*action_dim)\n",
    "            \n",
    "                qcurr = qvalue[index][0]\n",
    "                #print(qcurr)\n",
    "                qcurr[int(action)] = Qtarget \n",
    "                #print(qcurr)\n",
    "                X.append(state)\n",
    "                y.append(qcurr)\n",
    "            X, y = np.array(X).reshape(self.batch_size,1,self.states), np.array(y).reshape(self.batch_size, 1, self.n_actions)\n",
    "            \n",
    "            loss = self.model.Qpolicy.train_on_batch(X, y,return_dict=True)\n",
    "            \n",
    "\n",
    "\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space=[0,1]\n",
    "state_space_dqn=np.array([[[[i,j,k] for j in range(B)] for i in range(T)] for k in range(0,4)]).reshape(4*T*B,3)\n",
    "state_space_arm1=np.array([[[i,j,0] for j in range(B)] for i in range(T)]).reshape(T*B,3)\n",
    "state_space_arm2=np.array([[[i,j,1] for j in range(B)] for i in range(T)]).reshape(T*B,3)\n",
    "state_space_arm3=np.array([[[i,j,2] for j in range(B)] for i in range(T)]).reshape(T*B,3)\n",
    "state_space_arm4=np.array([[[i,j,3] for j in range(B)] for i in range(T)]).reshape(T*B,3)\n",
    "\n",
    "N=20\n",
    "M=5\n",
    "\n",
    "epsilon=1\n",
    "subsidy=0\n",
    "arm_indexes=[i for i in range(N)]\n",
    "gamma=0.9995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  0,  2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_space_arm3[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newArrival(state_space):\n",
    "    return np.array(random.sample(list(state_space), 1), dtype=np.float32).reshape(3,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.,  6.,  2.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newArrival(state_space_arm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action,state_space,processing_cost=0.8):\n",
    "        ''' function to calculate the reward and next state. '''\n",
    "        currentState = state\n",
    "\n",
    "        if action == 1:\n",
    "            if (currentState[1] >= 0) and (currentState[0] == 0): \n",
    "                reward = 0 \n",
    "                nextState = newArrival(state_space)\n",
    "            elif (currentState[1] >= 0) and (currentState[0] > 1): \n",
    "                reward = (1 - processing_cost)\n",
    "                currentState[0] -= 1\n",
    "                currentState[1] -= 1\n",
    "                if currentState[1] < 0:\n",
    "                    currentState[1] = 0\n",
    "                    reward = 0\n",
    "                nextState = np.array([currentState[0], currentState[1], currentState[2]],  dtype=np.float32)\n",
    "            elif (currentState[1] >= 0) and (currentState[0] == 1): \n",
    "                reward = ((1 - processing_cost) - 0.2*(((currentState[1]) - 1)**2)) \n",
    "                if (currentState[1] == 0):\n",
    "                    reward = 0\n",
    "                currentState[1] = 0\n",
    "                currentState[0] = 0\n",
    "                nextState = newArrival(state_space)\n",
    "\n",
    "        elif action == 0:\n",
    "            if (currentState[1] >= 0)  and (currentState[0] == 0):\n",
    "                reward = 0\n",
    "                nextState = newArrival(state_space)\n",
    "            elif (currentState[1] >= 0) and (currentState[0] > 1): \n",
    "                reward = 0\n",
    "                currentState[0] -= 1\n",
    "                nextState = np.array([currentState[0], currentState[1], currentState[2]], dtype=np.float32)\n",
    "            elif (currentState[1] >= 0) and (currentState[0] == 1):  \n",
    "                reward =  -0.2*(((currentState[1]))**2)  \n",
    "                currentState[1] = 0\n",
    "                currentState[0] = 0\n",
    "                nextState = newArrival(state_space)\n",
    "\n",
    "        \n",
    "        return nextState, reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 7. 0.]\n",
      "0.09999999999999998\n"
     ]
    }
   ],
   "source": [
    "nextState, reward=step(np.array([1,3,0]),1,state_space_arm1,processing_costs[0])\n",
    "print(nextState)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextState.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-05 02:37:15.347698: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-04-05 02:37:15.347769: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-04-05 02:37:15.347773: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-04-05 02:37:15.347924: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-04-05 02:37:15.348696: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "dqn_solver=DQNSolver(all_states=state_space_dqn,state_space=state_dim,action_space=action_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "newArrival_state_space_arm1=np.array([[[0,i,j] for j in range(B)] for i in range(1,T)]).reshape((T-1)*B,3)\n",
    "x=list(newArrival_state_space_arm1)\n",
    "x.append(np.array([0,0,0]))\n",
    "newArrival_state_space_arm1=np.array(x)\n",
    "newArrival_state_space_arm2=np.array([[[1,i,j] for j in range(B)] for i in range(1,T)]).reshape((T-1)*B,3)\n",
    "x=list(newArrival_state_space_arm2)\n",
    "x.append(np.array([1,0,0]))\n",
    "newArrival_state_space_arm2=np.array(x)\n",
    "newArrival_state_space_arm3=np.array([[[2,i,j] for j in range(B)] for i in range(1,T)]).reshape((T-1)*B,3)\n",
    "x=list(newArrival_state_space_arm3)\n",
    "x.append(np.array([2,0,0]))\n",
    "newArrival_state_space_arm3=np.array(x)\n",
    "newArrival_state_space_arm4=np.array([[[3,i,j] for j in range(B)] for i in range(1,T)]).reshape((T-1)*B,3)\n",
    "x=list(newArrival_state_space_arm4)\n",
    "x.append(np.array([3,0,0]))\n",
    "newArrival_state_space_arm4=np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state=[newArrival(newArrival_state_space_arm1) for _ in range(int(N/4))]\n",
    "current_state.extend([newArrival(newArrival_state_space_arm2) for _ in range(int(N/4))])\n",
    "current_state.extend([newArrival(newArrival_state_space_arm3) for _ in range(int(N/4))])\n",
    "current_state.extend([newArrival(newArrival_state_space_arm4) for _ in range(int(N/4))])\n",
    "\n",
    "rewards=[]\n",
    "subsidies=[]\n",
    "action_sums=[]\n",
    "betas=[]\n",
    "alphas=[]\n",
    "epsilon=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_each_arm(x):\n",
    "    current_state=x[0]\n",
    "    dqn_solver=x[1]\n",
    "    subsidy=x[2]\n",
    "    type=x[3]\n",
    "    state_space_arm1=np.array([[[i,j,0] for j in range(B)] for i in range(T)]).reshape(T*B,3)\n",
    "    state_space_arm2=np.array([[[i,j,1] for j in range(B)] for i in range(T)]).reshape(T*B,3)\n",
    "    state_space_arm3=np.array([[[i,j,2] for j in range(B)] for i in range(T)]).reshape(T*B,3)\n",
    "    state_space_arm4=np.array([[[i,j,3] for j in range(B)] for i in range(T)]).reshape(T*B,3)\n",
    "    newArrival_state_space_arm1=np.array([[[0,i,j] for j in range(B)] for i in range(1,T)]).reshape((T-1)*B,3)\n",
    "    x=list(newArrival_state_space_arm1)\n",
    "    x.append(np.array([0,0,0]))\n",
    "    newArrival_state_space_arm1=np.array(x)\n",
    "    newArrival_state_space_arm2=np.array([[[1,i,j] for j in range(B)] for i in range(1,T)]).reshape((T-1)*B,3)\n",
    "    x=list(newArrival_state_space_arm2)\n",
    "    x.append(np.array([1,0,0]))\n",
    "    newArrival_state_space_arm2=np.array(x)\n",
    "    newArrival_state_space_arm3=np.array([[[2,i,j] for j in range(B)] for i in range(1,T)]).reshape((T-1)*B,3)\n",
    "    x=list(newArrival_state_space_arm3)\n",
    "    x.append(np.array([2,0,0]))\n",
    "    newArrival_state_space_arm3=np.array(x)\n",
    "    newArrival_state_space_arm4=np.array([[[3,i,j] for j in range(B)] for i in range(1,T)]).reshape((T-1)*B,3)\n",
    "    x=list(newArrival_state_space_arm4)\n",
    "    x.append(np.array([3,0,0]))\n",
    "    newArrival_state_space_arm4=np.array(x)\n",
    "    s=np.reshape(current_state,(1,3))\n",
    "    action=dqn_solver.act(s)\n",
    "    if type==0:\n",
    "        x=step(current_state,action,state_space_arm1,newArrival_state_space_arm1,processing_cost=0.1)\n",
    "    elif type==1:\n",
    "        x=step(current_state,action,state_space_arm2,newArrival_state_space_arm2,processing_cost=0.3)\n",
    "    elif type==2:\n",
    "        x=step(current_state,action,state_space_arm3,newArrival_state_space_arm3,processing_cost=0.6)\n",
    "    else:\n",
    "        x=step(current_state,action,state_space_arm4,newArrival_state_space_arm4,processing_cost=0.8)\n",
    "    new_state=x[0]\n",
    "    s_=np.reshape(new_state,(1,3))\n",
    "    reward=x[1]\n",
    "    dqn_solver.remember(s, action, reward, s_, False,subsidy)\n",
    "    dqn_solver.train()\n",
    "    return action,current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j=15\n",
    "j%(N/4)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,150001):\n",
    "    action_sum=0\n",
    "    reward_sum=0\n",
    "    beta=1/(1+np.ceil(i*np.log(i)/5000))\n",
    "    \n",
    "    all=[]\n",
    "    type=-1\n",
    "    for j in range(len(current_state)):\n",
    "        if j%(N/4)==0:\n",
    "            type=type+1\n",
    "        all.append([current_state[j],dqn_solver,subsidy,type])\n",
    "\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures=[executor.submit(for_each_arm,x) for x in all]\n",
    "        results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "    \n",
    "    for k,result in enumerate(results):\n",
    "        action_sum+=result[0]\n",
    "        current_state[k]=result[1]\n",
    "    if i%100==0:\n",
    "        print(i)\n",
    "    dqn_solver.update_target_model()\n",
    "    dqn_solver.decrement_epsilon()\n",
    "    subsidy+=beta*(action_sum-M)\n",
    "    subsidies.append(subsidy)\n",
    "    rewards.append(reward_sum)\n",
    "    action_sums.append(action_sum)\n",
    "    betas.append(beta)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_solver.epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(subsidies)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Subsidy')\n",
    "plt.title('Subsidy vs Steps for Deadline Scehduling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['action_sums']=action_sums\n",
    "plt.plot(df['action_sums'].rolling(100000,min_periods=1).mean())\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Number of Arms activated')\n",
    "plt.title(' Moving Average of number of arms activated for Deadline Scehduling ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
