{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_0_matrix(x):\n",
    "    P_0_matrix=np.array([[1-x,x,0,0,0],\n",
    "                   [1-x,0,x,0,0],\n",
    "                   [1-x,0,0,x,0],\n",
    "                   [1-x,0,0,0,x],\n",
    "                   [1-x,0,0,0,x]])  \n",
    "    return P_0_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_1_matrix=np.array([[1,0,0,0,0],\n",
    "                   [1,0,0,0,0],\n",
    "                   [1,0,0,0,0],\n",
    "                   [1,0,0,0,0],\n",
    "                   [1,0,0,0,0]])\n",
    "P_1_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_matrix(x):\n",
    "    reward_matrix=[[x**1,0],[x**2,0],[x**3,0],[x**4,0],[x**5,0]]\n",
    "    return reward_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space=[0,1]\n",
    "state_space=[0,1,2,3,4]\n",
    "classes=2\n",
    "N=15 # 10 of class 1 and 5 of class 2\n",
    "M=6\n",
    "epsilon=0.1\n",
    "subsidy=0\n",
    "arm_indexes=[i for i in range(N)]\n",
    "gamma=0.99\n",
    "prob=M/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state,state_space,action,P_0_matrix,P_1_matrix,reward_matrix):\n",
    "  new_state=state\n",
    "  if(action==1):\n",
    "    new_state=np.random.choice(state_space,replace=True,p=P_1_matrix[state])\n",
    "    reward=reward_matrix[state][1]\n",
    "  else:\n",
    "    new_state=np.random.choice(state_space,replace=True,p=P_0_matrix[state])\n",
    "    reward=reward_matrix[state][0]\n",
    "  return [new_state,reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.1, 0],\n",
       "  [0.010000000000000002, 0],\n",
       "  [0.0010000000000000002, 0],\n",
       "  [0.00010000000000000002, 0],\n",
       "  [1.0000000000000003e-05, 0]],\n",
       " [[0.2, 0],\n",
       "  [0.04000000000000001, 0],\n",
       "  [0.008000000000000002, 0],\n",
       "  [0.0016000000000000003, 0],\n",
       "  [0.0003200000000000001, 0]]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values=[]\n",
    "for i in range(0,classes):\n",
    "  Q_values.append(reward_matrix((i+0.8)/10.0))\n",
    "  \n",
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_value_update(Q_values,state,next_state,action,reward,arm_index,subsidy,alpha):\n",
    "  if action==1:\n",
    "    Q_values[arm_index][state][action]=(1-alpha)*(Q_values[arm_index][state][action])+alpha*(reward+max(Q_values[arm_index][next_state])-(np.array(Q_values[arm_index]).sum())/10)\n",
    "    return Q_values\n",
    "\n",
    "  else:\n",
    "    Q_values[arm_index][state][action]=(1-alpha)*(Q_values[arm_index][state][action])+alpha*(reward+subsidy+max(Q_values[arm_index][next_state])-(np.array(Q_values[arm_index]).sum())/10)\n",
    "    return Q_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act(Q_values,epsilon,state,arm_index):\n",
    "  decision=np.random.binomial(n=1,p=epsilon,size=1)\n",
    "  if decision==1:\n",
    "    action=random.choice([0,1])\n",
    "  else:\n",
    "    action=np.argmax(Q_values[arm_index][state])\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternative_indexes=[0 for _ in range(N)]\n",
    "current_state=[0 for _ in range(classes)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu=np.zeros((N,len(state_space),len(action_space)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards=[]\n",
    "subsidies=[]\n",
    "action_avgs=[]\n",
    "betas=[]\n",
    "alphas=[]\n",
    "epsilon=0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(i):\n",
    "    return 1/np.ceil(1+(i/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_sum=0\n",
    "action_sum=[0 for _ in range(classes)]\n",
    "total_action_sum=0\n",
    "for i in range(1,100000001):\n",
    "    # top_M_indices=act(alternative_indexes,N,M,epsilon)\n",
    "    # alpha=1/(1+(i/500))\n",
    "    if i%50==0:\n",
    "        beta=1/(1+np.ceil(i*np.log(i)/10000))\n",
    "    else:\n",
    "      beta=0\n",
    "    action_sum_flag=0\n",
    "\n",
    "    for index in range(classes):\n",
    "        action=act(Q_values,epsilon,current_state[index],index)\n",
    "        action_sum[index]+=action\n",
    "\n",
    "\n",
    "        x=step(current_state[index],state_space,action,P_0_matrix((index+0.8)/10.0),P_1_matrix,reward_matrix((index+0.8)/10.0))\n",
    "        new_state=x[0]\n",
    "        reward=x[1]\n",
    "\n",
    "        Q_value_update(Q_values,current_state[index],new_state,action,reward,index,subsidy,alpha(nu[index][current_state[index],action]))\n",
    "        nu[index][current_state[index],action]=nu[index][current_state[index],action]+1\n",
    "        current_state[index]=new_state\n",
    "        reward_sum+=reward\n",
    "        \n",
    "        #print(x)\n",
    "    # print(i)\n",
    "    #print(reward_sum)\n",
    "    total_action_sum=sum(action_sum)\n",
    "    subsidy+=beta*(2*action_sum[0]/(3*i)+action_sum[0]/(3*i)-prob)\n",
    "    subsidies.append(subsidy)\n",
    "    # epsilon=epsilon*gamma\n",
    "    # if epsilon<=0.01:\n",
    "    #     epsilon=0.01\n",
    "    rewards.append(reward_sum/i)\n",
    "    action_avgs.append(total_action_sum/i)\n",
    "    # betas.append(beta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
